{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Load a local model (e.g., Mistral)\n",
    "llm = Ollama(model=\"llama3.3:latest\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B6r7Sv4DH6Dal9MDfOx20R7yefv0X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1740973858, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=12, total_tokens=18, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OAK')\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OAK'),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "initial_csv = pd.read_csv(\"./csv/Explore-logs-A-data-2025-03-02 22_52_03.csv\")\n",
    "\n",
    "# print(initial_csv.iloc[:, :2])\n",
    "\n",
    "initial_csv.iloc[:, :2].to_csv('./csv/processed_logs.csv', index=False)\n",
    "\n",
    "# json_string = r\"\"\"{\"level\":50,\"time\":1740573577228,\"pid\":1,\"hostname\":\"amaginow\",\"reqId\":\"9803d689-39df-4e7c-83a9-44d5aa0c40e3\",\"err\":{\"message\":\"Request failed with status code 404\",\"name\":\"AxiosError\",\"stack\":\"AxiosError: Request failed with status code 404\\n    at settle (/home/node/app/server.js:89512:12)\\n    at IncomingMessage.handleStreamEnd (/home/node/app/server.js:91460:11)\\n    at IncomingMessage.emit (node:events:532:35)\\n    at endReadableNT (node:internal/streams/readable:1696:12)\\n    at process.processTicksAndRejections (node:internal/process/task_queues:90:21)\\n    at Axios.request (/home/node/app/server.js:92610:41)\\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\\n    at async Object.getDistributions (/home/node/app/server.js:93191:43)\",\"config\":{\"transitional\":{\"silentJSONParsing\":true,\"forcedJSONParsing\":true,\"clarifyTimeoutError\":false},\"adapter\":[\"xhr\",\"http\",\"fetch\"],\"transformRequest\":[null],\"transformResponse\":[null],\"timeout\":0,\"xsrfCookieName\":\"XSRF-TOKEN\",\"xsrfHeaderName\":\"X-XSRF-TOKEN\",\"maxContentLength\":-1,\"maxBodyLength\":-1,\"env\":{},\"headers\":{\"Accept\":\"application/json, text/plain, */*\",\"x-account-id\":\"amg78787\",\"Traceparent\":\"9803d689-39df-4e7c-83a9-44d5aa0c40e3\",\"User-Agent\":\"axios/1.7.7\",\"Accept-Encoding\":\"gzip, compress, deflate, br\"},\"params\":{\"per_page\":150,\"page\":1,\"order\":\"desc\",\"search\":\"\",\"channelid\":\"\"},\"method\":\"get\",\"url\":\"http://fabric.amaginow.svc.cluster.local:5100//api/v3/deliveries\"},\"code\":\"ERR_BAD_REQUEST\",\"status\":404},\"msg\":\"Request failed with status code 404\"}\"\"\"\n",
    "\n",
    "# json_object = json.loads(json_string)\n",
    "\n",
    "# print(json_object)  # Now it's a Python dictionary\n",
    "# print(json_object['err']['message'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: What are the most common error types in the logs?\n",
      "Added user message to memory: What are the most common error types in the logs?\n",
      "=== Calling Function ===\n",
      "Calling function: count_errors_by_type with args: {}\n",
      "Got output: {'AxiosError': 26}\n",
      "========================\n",
      "\n",
      "Analyzing: Analyze the error patterns and suggest potential causes.\n",
      "Added user message to memory: Analyze the error patterns and suggest potential causes.\n",
      "=== Calling Function ===\n",
      "Calling function: generate_error_summary with args: {}\n",
      "Got output: {'total_errors': 26, 'error_types': {'AxiosError': 26}, 'status_codes': {404: 26}, 'top_url_status_combinations': {('http://fabric.amaginow.svc.cluster.local:5100//api/v3/deliveries', 404): 26}, 'channel_id_frequencies': {'amg78787c7': 9, 'amg78787c6': 6, '': 5, 'amg78787c8': 4, 'amg78787c9': 2}}\n",
      "========================\n",
      "\n",
      "Analyzing: Are there specific channel IDs that experience more errors?\n",
      "Added user message to memory: Are there specific channel IDs that experience more errors?\n",
      "Analyzing: Can you identify any potential API endpoint issues based on the logs?\n",
      "Added user message to memory: Can you identify any potential API endpoint issues based on the logs?\n",
      "=== Calling Function ===\n",
      "Calling function: find_most_frequent_urls with args: {}\n",
      "Got output: {'http://fabric.amaginow.svc.cluster.local:5100//api/v3/deliveries': 26}\n",
      "========================\n",
      "\n",
      "Analyzing: What can you tell me about the stack traces in these errors?\n",
      "Added user message to memory: What can you tell me about the stack traces in these errors?\n",
      "=== Calling Function ===\n",
      "Calling function: analyze_stack_traces with args: {}\n",
      "Got output: {}\n",
      "========================\n",
      "\n",
      "Analyzing: What are the most common error patterns in these logs?\n",
      "Added user message to memory: What are the most common error patterns in these logs?\n",
      "=== Calling Function ===\n",
      "Calling function: generate_error_summary with args: {}\n",
      "Got output: {'total_errors': 26, 'error_types': {'AxiosError': 26}, 'status_codes': {404: 26}, 'top_url_status_combinations': {('http://fabric.amaginow.svc.cluster.local:5100//api/v3/deliveries', 404): 26}, 'channel_id_frequencies': {'amg78787c7': 9, 'amg78787c6': 6, '': 5, 'amg78787c8': 4, 'amg78787c9': 2}}\n",
      "========================\n",
      "\n",
      "Analyzing: What might be causing these errors based on the data?\n",
      "Added user message to memory: What might be causing these errors based on the data?\n",
      "Analyzing: What recommendations would you make to fix these issues?\n",
      "Added user message to memory: What recommendations would you make to fix these issues?\n",
      "Analyzing: Are there any unusual patterns worth investigating further?\n",
      "Added user message to memory: Are there any unusual patterns worth investigating further?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 446\u001b[39m\n\u001b[32m    443\u001b[39m results = logs_system.run_batch_analysis(questions)\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# Option 3: Generate comprehensive report\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[43mlogs_system\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_comprehensive_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 409\u001b[39m, in \u001b[36mLogAnalysisSystem.generate_comprehensive_report\u001b[39m\u001b[34m(self, output_file)\u001b[39m\n\u001b[32m    380\u001b[39m         insights = \u001b[38;5;28mself\u001b[39m.run_batch_analysis(standard_questions)\n\u001b[32m    382\u001b[39m         \u001b[38;5;66;03m# Create report content\u001b[39;00m\n\u001b[32m    383\u001b[39m         report = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m# Microservice Log Analysis Report\u001b[39m\n\u001b[32m    384\u001b[39m \n\u001b[32m    385\u001b[39m \u001b[33m## Summary Statistics\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[33m- Total Errors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[33m'\u001b[39m\u001b[33mtotal_errors\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[33m- Unique Error Types: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(summary[\u001b[33m'\u001b[39m\u001b[33merror_types\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[33m- Status Codes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[33m'\u001b[39m\u001b[33mstatus_codes\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    389\u001b[39m \n\u001b[32m    390\u001b[39m \u001b[33m## Error Patterns\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;132;01m{\u001b[39;00minsights[standard_questions[\u001b[32m0\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    392\u001b[39m \n\u001b[32m    393\u001b[39m \u001b[33m## Probable Causes\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;132;01m{\u001b[39;00minsights[standard_questions[\u001b[32m1\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    395\u001b[39m \n\u001b[32m    396\u001b[39m \u001b[33m## Recommendations\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;132;01m{\u001b[39;00minsights[standard_questions[\u001b[32m2\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    398\u001b[39m \n\u001b[32m    399\u001b[39m \u001b[33m## Unusual Patterns\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;132;01m{\u001b[39;00minsights[standard_questions[\u001b[32m3\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    401\u001b[39m \n\u001b[32m    402\u001b[39m \u001b[33m## Visualizations\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[33m- Error Frequency Over Time: error_time_series.png\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[33m- Error Types Distribution: error_types_chart.png\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[33m- HTTP Status Codes: status_codes_chart.png\u001b[39m\n\u001b[32m    406\u001b[39m \n\u001b[32m    407\u001b[39m \u001b[33m## Detailed Error Breakdown\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m    411\u001b[39m \n\u001b[32m    412\u001b[39m \u001b[33m## Stack Trace Analysis\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mjson.dumps(\u001b[38;5;28mself\u001b[39m.analyzer.analyze_stack_traces(),\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33m```\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[38;5;66;03m# Write report to file\u001b[39;00m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/json/__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/json/encoder.py:202\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    200\u001b[39m chunks = \u001b[38;5;28mself\u001b[39m.iterencode(o, _one_shot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     chunks = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/json/encoder.py:377\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mkeys must be str, int, float, bool or None, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    378\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[32m    380\u001b[39m     first = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: keys must be str, int, float, bool or None, not tuple"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "# from llama_index import VectorStoreIndex, Document, ServiceContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "class LogParser:\n",
    "    \"\"\"Class for parsing and structuring log data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_csv(file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load log data from CSV and parse the JSON content.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Parse JSON strings in the 'Line' column\n",
    "        parsed_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            timestamp = int(row['Time'])\n",
    "            try:\n",
    "                log_entry = json.loads(row['Line'])\n",
    "                log_entry['timestamp'] = timestamp\n",
    "                parsed_data.append(log_entry)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON at timestamp {timestamp}\")\n",
    "        \n",
    "        return pd.DataFrame(parsed_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def structure_logs(logs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract and structure relevant information from the logs.\"\"\"\n",
    "        structured_logs = []\n",
    "        \n",
    "        for _, log in logs_df.iterrows():\n",
    "            entry = {\n",
    "                'timestamp': log['timestamp'],\n",
    "                'datetime': datetime.fromtimestamp(log['timestamp']/1000).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'level': log.get('level'),\n",
    "                'message': log.get('msg'),\n",
    "                'request_id': log.get('reqId'),\n",
    "                'error_name': log.get('err', {}).get('name'),\n",
    "                'error_message': log.get('err', {}).get('message'),\n",
    "                'error_code': log.get('err', {}).get('code'),\n",
    "                'error_status': log.get('err', {}).get('status'),\n",
    "            }\n",
    "            \n",
    "            # Extract API details from config if available\n",
    "            if 'err' in log and 'config' in log['err']:\n",
    "                config = log['err']['config']\n",
    "                entry['api_url'] = config.get('url')\n",
    "                entry['api_method'] = config.get('method')\n",
    "                \n",
    "                if 'params' in config:\n",
    "                    entry['api_params'] = config['params']\n",
    "            \n",
    "            structured_logs.append(entry)\n",
    "        \n",
    "        return pd.DataFrame(structured_logs)\n",
    "\n",
    "\n",
    "class LogAnalyzer:\n",
    "    \"\"\"Class for analyzing structured log data\"\"\"\n",
    "    \n",
    "    def __init__(self, logs_df: pd.DataFrame):\n",
    "        self.logs_df = logs_df\n",
    "    \n",
    "    def count_errors_by_type(self) -> Dict[str, int]:\n",
    "        \"\"\"Count errors by type/name.\"\"\"\n",
    "        return self.logs_df['error_name'].value_counts().to_dict()\n",
    "    \n",
    "    def count_errors_by_status(self) -> Dict[int, int]:\n",
    "        \"\"\"Count errors by HTTP status code.\"\"\"\n",
    "        return self.logs_df['error_status'].value_counts().to_dict()\n",
    "    \n",
    "    def find_most_frequent_urls(self) -> Dict[str, int]:\n",
    "        \"\"\"Find the most frequently occurring URLs in error logs.\"\"\"\n",
    "        return self.logs_df['api_url'].value_counts().to_dict()\n",
    "    \n",
    "    def errors_over_time(self, interval: str = '1min') -> Dict[datetime, int]:\n",
    "        \"\"\"Analyze error frequency over time.\"\"\"\n",
    "        df_copy = self.logs_df.copy()\n",
    "        df_copy['datetime'] = pd.to_datetime(df_copy['datetime'])\n",
    "        df_copy = df_copy.set_index('datetime')\n",
    "        error_counts = df_copy.resample(interval).size()\n",
    "        return error_counts.to_dict()\n",
    "    \n",
    "    def search_logs_by_request_id(self, request_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for logs with a specific request ID.\"\"\"\n",
    "        matching_logs = self.logs_df[self.logs_df['request_id'] == request_id]\n",
    "        return matching_logs.to_dict('records')\n",
    "    \n",
    "    def search_logs_by_channel_id(self, channel_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for logs related to a specific channel ID.\"\"\"\n",
    "        matching_logs = self.logs_df[self.logs_df['api_params'].apply(\n",
    "            lambda x: isinstance(x, dict) and x.get('channelid') == channel_id\n",
    "        )]\n",
    "        return matching_logs.to_dict('records')\n",
    "    \n",
    "    def generate_error_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a summary of error patterns in the logs.\"\"\"\n",
    "        total_errors = len(self.logs_df)\n",
    "        error_types = self.count_errors_by_type()\n",
    "        status_codes = self.count_errors_by_status()\n",
    "        \n",
    "        # Check for duplicate errors (same URL, same status)\n",
    "        url_status_counts = self.logs_df.groupby(['api_url', 'error_status']).size().sort_values(ascending=False)\n",
    "        \n",
    "        # Check for patterns in parameters\n",
    "        channel_counts = self.logs_df['api_params'].apply(\n",
    "            lambda x: x.get('channelid') if isinstance(x, dict) else None\n",
    "        ).value_counts().to_dict()\n",
    "        \n",
    "        return {\n",
    "            \"total_errors\": total_errors,\n",
    "            \"error_types\": error_types,\n",
    "            \"status_codes\": status_codes,\n",
    "            \"top_url_status_combinations\": url_status_counts.to_dict(),\n",
    "            \"channel_id_frequencies\": channel_counts\n",
    "        }\n",
    "    \n",
    "    def analyze_stack_traces(self) -> Dict[str, int]:\n",
    "        \"\"\"Extract common patterns from stack traces.\"\"\"\n",
    "        stack_trace_components = []\n",
    "        for _, log in self.logs_df.iterrows():\n",
    "            if 'err' in log and 'stack' in log['err']:\n",
    "                stack = log['err']['stack']\n",
    "                # Extract key components like function names and line numbers\n",
    "                components = [line.strip() for line in stack.split('\\n') if line.strip()]\n",
    "                stack_trace_components.extend(components)\n",
    "        \n",
    "        component_counts = pd.Series(stack_trace_components).value_counts().head(10).to_dict()\n",
    "        return component_counts\n",
    "\n",
    "\n",
    "class LogVisualizer:\n",
    "    \"\"\"Class for visualizing log data\"\"\"\n",
    "    \n",
    "    def __init__(self, logs_df: pd.DataFrame):\n",
    "        self.logs_df = logs_df\n",
    "    \n",
    "    # def errors_over_time(self, output_path: str = 'error_frequency.png') -> str:\n",
    "    #     \"\"\"Generate a visualization of errors over time.\"\"\"\n",
    "    #     df_copy = self.logs_df.copy()\n",
    "    #     df_copy['datetime'] = pd.to_datetime(df_copy['datetime'])\n",
    "    #     df_copy = df_copy.set_index('datetime')\n",
    "    #     error_counts = df_copy.resample('1min').size()\n",
    "        \n",
    "    #     plt.figure(figsize=(12, 6))\n",
    "    #     error_counts.plot(kind='line')\n",
    "    #     plt.title('Error Frequency Over Time')\n",
    "    #     plt.xlabel('Time')\n",
    "    #     plt.ylabel('Number of Errors')\n",
    "    #     plt.tight_layout()\n",
    "        \n",
    "    #     # Save the plot to a file\n",
    "    #     plt.savefig(output_path)\n",
    "    #     plt.close()\n",
    "        \n",
    "    #     return f\"Visualization saved to {output_path}\"\n",
    "    \n",
    "    # def error_types_pie_chart(self, output_path: str = 'error_types.png') -> str:\n",
    "    #     \"\"\"Generate a pie chart of error types.\"\"\"\n",
    "    #     error_types = self.logs_df['error_name'].value_counts()\n",
    "        \n",
    "    #     plt.figure(figsize=(10, 10))\n",
    "    #     plt.pie(error_types, labels=error_types.index, autopct='%1.1f%%')\n",
    "    #     plt.title('Distribution of Error Types')\n",
    "    #     plt.tight_layout()\n",
    "        \n",
    "    #     plt.savefig(output_path)\n",
    "    #     plt.close()\n",
    "        \n",
    "    #     return f\"Visualization saved to {output_path}\"\n",
    "    \n",
    "    # def status_code_bar_chart(self, output_path: str = 'status_codes.png') -> str:\n",
    "    #     \"\"\"Generate a bar chart of HTTP status codes.\"\"\"\n",
    "    #     status_codes = self.logs_df['error_status'].value_counts()\n",
    "        \n",
    "    #     plt.figure(figsize=(12, 6))\n",
    "    #     status_codes.plot(kind='bar')\n",
    "    #     plt.title('HTTP Status Code Distribution')\n",
    "    #     plt.xlabel('Status Code')\n",
    "    #     plt.ylabel('Count')\n",
    "    #     plt.tight_layout()\n",
    "        \n",
    "    #     plt.savefig(output_path)\n",
    "    #     plt.close()\n",
    "        \n",
    "    #     return f\"Visualization saved to {output_path}\"\n",
    "\n",
    "\n",
    "class LogAnalysisAgent:\n",
    "    \"\"\"Class for managing the log analysis agent powered by LlamaIndex and OpenAI\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer: LogAnalyzer, visualizer: LogVisualizer, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.analyzer = analyzer\n",
    "        self.visualizer = visualizer\n",
    "        self.model = model\n",
    "        self.agent = self._setup_agent()\n",
    "    \n",
    "    def _setup_agent(self) -> OpenAIAgent:\n",
    "        \"\"\"Set up the LlamaIndex agent with analysis tools.\"\"\"\n",
    "        \n",
    "        # Create function tools\n",
    "        tools = [\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=self.analyzer.count_errors_by_type,\n",
    "                name=\"count_errors_by_type\",\n",
    "                description=\"Count errors by their type/name\"\n",
    "            ),\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=self.analyzer.count_errors_by_status,\n",
    "                name=\"count_errors_by_status\",\n",
    "                description=\"Count errors by HTTP status code\"\n",
    "            ),\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=self.analyzer.find_most_frequent_urls,\n",
    "                name=\"find_most_frequent_urls\",\n",
    "                description=\"Find the most frequently occurring URLs in error logs\"\n",
    "            ),\n",
    "            # FunctionTool.from_defaults(\n",
    "            #     fn=lambda interval='1min': self.analyzer.errors_over_time(interval),\n",
    "            #     name=\"errors_over_time\",\n",
    "            #     description=\"Analyze error frequency over time with specified interval (e.g., '1min', '5min', '1h')\"\n",
    "            # ),\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=lambda request_id: self.analyzer.search_logs_by_request_id(request_id),\n",
    "                name=\"search_logs_by_request_id\",\n",
    "                description=\"Search for logs with a specific request ID\"\n",
    "            ),\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=lambda channel_id: self.analyzer.search_logs_by_channel_id(channel_id),\n",
    "                name=\"search_logs_by_channel_id\",\n",
    "                description=\"Search for logs related to a specific channel ID\"\n",
    "            ),\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=self.analyzer.generate_error_summary,\n",
    "                name=\"generate_error_summary\",\n",
    "                description=\"Generate a summary of error patterns in the logs\"\n",
    "            ),\n",
    "            FunctionTool.from_defaults(\n",
    "                fn=self.analyzer.analyze_stack_traces,\n",
    "                name=\"analyze_stack_traces\",\n",
    "                description=\"Extract common patterns from stack traces\"\n",
    "            ),\n",
    "            # FunctionTool.from_defaults(\n",
    "            #     fn=self.visualizer.errors_over_time,\n",
    "            #     name=\"visualize_errors_over_time\",\n",
    "            #     description=\"Generate a visualization of errors over time\"\n",
    "            # ),\n",
    "            # FunctionTool.from_defaults(\n",
    "            #     fn=self.visualizer.error_types_pie_chart,\n",
    "            #     name=\"visualize_error_types\",\n",
    "            #     description=\"Generate a pie chart of error types\"\n",
    "            # ),\n",
    "            # FunctionTool.from_defaults(\n",
    "            #     fn=self.visualizer.status_code_bar_chart,\n",
    "            #     name=\"visualize_status_codes\",\n",
    "            #     description=\"Generate a bar chart of HTTP status codes\"\n",
    "            # )\n",
    "        ]\n",
    "        \n",
    "        # Create OpenAI-based service context and agent\n",
    "        llm = OpenAI(model=self.model)\n",
    "        \n",
    "        # Create the agent\n",
    "        agent = OpenAIAgent.from_tools(\n",
    "            tools,\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            system_prompt=\"\"\"\n",
    "            You are a log analysis expert. Your job is to help the user understand patterns and issues in their \n",
    "            microservice logs. Use the available tools to analyze the log data and provide insights.\n",
    "            \n",
    "            Focus on:\n",
    "            1. Identifying error patterns\n",
    "            2. Finding the root causes of errors\n",
    "            3. Suggesting potential fixes\n",
    "            4. Highlighting unusual behavior\n",
    "            \n",
    "            Always show your reasoning and be specific in your recommendations.\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    def chat(self, question: str) -> Any:\n",
    "        \"\"\"Query the agent with a question about the logs.\"\"\"\n",
    "        return self.agent.chat(question)\n",
    "    \n",
    "    # def create_vector_index(self, sample_size: int = 50) -> VectorStoreIndex:\n",
    "    #     \"\"\"Create a vector index for more complex queries (optional feature).\"\"\"\n",
    "    #     logs_df = self.analyzer.logs_df\n",
    "    #     llm = OpenAI(model=self.model)\n",
    "    #     service_context = ServiceContext.from_defaults(llm=llm)\n",
    "        \n",
    "    #     # Sample logs to create documents\n",
    "    #     sample_size = min(sample_size, len(logs_df))\n",
    "    #     documents = [Document(text=str(row.to_dict())) for _, row in logs_df.sample(sample_size).iterrows()]\n",
    "        \n",
    "    #     return VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "\n",
    "class LogAnalysisSystem:\n",
    "    \"\"\"Main class for the log analysis system\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, file_path: str, model: str = \"gpt-3.5-turbo\") -> \"LogAnalysisSystem\":\n",
    "        \"\"\"Create a log analysis system from a CSV file.\"\"\"\n",
    "        # Parse logs\n",
    "        raw_logs_df = LogParser.from_csv(file_path)\n",
    "        structured_logs_df = LogParser.structure_logs(raw_logs_df)\n",
    "        \n",
    "        # Create analyzer and visualizer\n",
    "        analyzer = LogAnalyzer(structured_logs_df)\n",
    "        visualizer = LogVisualizer(structured_logs_df)\n",
    "        \n",
    "        # Create agent\n",
    "        agent = LogAnalysisAgent(analyzer, visualizer, model)\n",
    "        \n",
    "        return cls(structured_logs_df, analyzer, visualizer, agent)\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        logs_df: pd.DataFrame, \n",
    "        analyzer: LogAnalyzer, \n",
    "        visualizer: LogVisualizer, \n",
    "        agent: LogAnalysisAgent\n",
    "    ):\n",
    "        self.logs_df = logs_df\n",
    "        self.analyzer = analyzer\n",
    "        self.visualizer = visualizer\n",
    "        self.agent = agent\n",
    "    \n",
    "    def run_interactive_session(self) -> None:\n",
    "        \"\"\"Run an interactive session with the log analysis agent.\"\"\"\n",
    "        print(\"Log Analysis Agent is ready! Type 'exit' to end the session.\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nWhat would you like to know about your logs? \")\n",
    "            if question.lower() == 'exit':\n",
    "                break\n",
    "            \n",
    "            print(\"Analyzing...\")\n",
    "            response = self.agent.chat(question)\n",
    "            print(f\"Response: {response}\")\n",
    "    \n",
    "    def run_batch_analysis(self, questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run batch analysis with a list of questions.\"\"\"\n",
    "        results = {}\n",
    "        for question in questions:\n",
    "            print(f\"Analyzing: {question}\")\n",
    "            response = self.agent.chat(question)\n",
    "            results[question] = response\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_comprehensive_report(self, output_file: str = \"log_analysis_report.md\") -> str:\n",
    "        \"\"\"Generate a comprehensive analysis report.\"\"\"\n",
    "        # Get basic statistics\n",
    "        summary = self.analyzer.generate_error_summary()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        # time_viz = self.visualizer.errors_over_time(\"error_time_series.png\")\n",
    "        # types_viz = self.visualizer.error_types_pie_chart(\"error_types_chart.png\")\n",
    "        # status_viz = self.visualizer.status_code_bar_chart(\"status_codes_chart.png\")\n",
    "        \n",
    "        # Ask the agent for insights\n",
    "        standard_questions = [\n",
    "            \"What are the most common error patterns in these logs?\",\n",
    "            \"What might be causing these errors based on the data?\",\n",
    "            \"What recommendations would you make to fix these issues?\",\n",
    "            \"Are there any unusual patterns worth investigating further?\"\n",
    "        ]\n",
    "        \n",
    "        insights = self.run_batch_analysis(standard_questions)\n",
    "        \n",
    "        # Create report content\n",
    "        report = f\"\"\"# Microservice Log Analysis Report\n",
    "\n",
    "## Summary Statistics\n",
    "- Total Errors: {summary['total_errors']}\n",
    "- Unique Error Types: {len(summary['error_types'])}\n",
    "- Status Codes: {summary['status_codes']}\n",
    "\n",
    "## Error Patterns\n",
    "{insights[standard_questions[0]]}\n",
    "\n",
    "## Probable Causes\n",
    "{insights[standard_questions[1]]}\n",
    "\n",
    "## Recommendations\n",
    "{insights[standard_questions[2]]}\n",
    "\n",
    "## Unusual Patterns\n",
    "{insights[standard_questions[3]]}\n",
    "\n",
    "## Visualizations\n",
    "- Error Frequency Over Time: error_time_series.png\n",
    "- Error Types Distribution: error_types_chart.png\n",
    "- HTTP Status Codes: status_codes_chart.png\n",
    "\n",
    "## Detailed Error Breakdown\n",
    "```\n",
    "{json.dumps(summary, indent=2)}\n",
    "```\n",
    "\n",
    "## Stack Trace Analysis\n",
    "```\n",
    "{json.dumps(self.analyzer.analyze_stack_traces(), indent=2)}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Write report to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        return f\"Comprehensive report generated at {output_file}\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the system\n",
    "    logs_system = LogAnalysisSystem.from_csv(\"./csv/processed_logs.csv\")\n",
    "    \n",
    "    # Option 1: Run interactive session\n",
    "    # logs_system.run_interactive_session()\n",
    "    \n",
    "    # Option 2: Run batch analysis\n",
    "    questions = [\n",
    "        \"What are the most common error types in the logs?\",\n",
    "        \"Analyze the error patterns and suggest potential causes.\",\n",
    "        \"Are there specific channel IDs that experience more errors?\",\n",
    "        # \"What's the distribution of errors over time?\",\n",
    "        \"Can you identify any potential API endpoint issues based on the logs?\",\n",
    "        # \"Generate a comprehensive summary of all errors.\",\n",
    "        # \"What can you tell me about the stack traces in these errors?\"\n",
    "    ]\n",
    "    results = logs_system.run_batch_analysis(questions)\n",
    "    \n",
    "    # Option 3: Generate comprehensive report\n",
    "    logs_system.generate_comprehensive_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
