{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Load a local model (e.g., Mistral)\n",
    "llm = Ollama(model=\"phi4:latest\", temperature=0.2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) - 32385 MiB free\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 579 tensors from /Users/idks/.ollama/models/blobs/sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 14B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 14B\n",
      "llama_model_loader: - kv   5:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type q4_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 8.37 GiB (4.87 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151644 '<｜User｜>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151647 '<|EOT|>' is not marked as EOG\n",
      "load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151645 '<｜Assistant｜>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 48\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 13824\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 14B\n",
      "print_info: model params     = 14.77 B\n",
      "print_info: general.name     = DeepSeek R1 Distill Qwen 14B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
      "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: PAD token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: layer  37 assigned to device CPU\n",
      "load_tensors: layer  38 assigned to device CPU\n",
      "load_tensors: layer  39 assigned to device CPU\n",
      "load_tensors: layer  40 assigned to device CPU\n",
      "load_tensors: layer  41 assigned to device CPU\n",
      "load_tensors: layer  42 assigned to device CPU\n",
      "load_tensors: layer  43 assigned to device CPU\n",
      "load_tensors: layer  44 assigned to device CPU\n",
      "load_tensors: layer  45 assigned to device CPU\n",
      "load_tensors: layer  46 assigned to device CPU\n",
      "load_tensors: layer  47 assigned to device Metal\n",
      "load_tensors: layer  48 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 566 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   166.42 MiB, ( 4644.62 / 36864.00)\n",
      "load_tensors: offloading 1 repeating layers to GPU\n",
      "load_tensors: offloaded 1/49 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =   166.42 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  8399.62 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 16384\n",
      "llama_init_from_model: n_ctx_per_seq = 16384\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4 Pro\n",
      "ggml_metal_init: picking default device: Apple M4 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x12ba75d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x6bfeba2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x12c264280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x12ba737b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x6c078a030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x6c0789780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x127f899f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x12ba72040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x12c264ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x127f8bdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x17eaadcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x12c265480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12c265ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x12c266b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x127f2ebd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x12ba6e1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12c266ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x12c267950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x12ba72700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x12ba6fc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x12c2683e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x127f2ee90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x127f2f150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x127f2f690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x12c2467a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x127f2fa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c247230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x127f302f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x6c0789080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x127f30bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x6baa7ef30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x12ba6ff40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x12ba74c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ba712f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c2484c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ba73e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x6baa7f7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c247e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x6baa80070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c248810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ba76970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ba77240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c2490b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ba77af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c249de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127f31020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ba78850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ba790c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c24a1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x6baa80500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c24b810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ba79990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x12c24c0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x6baa80910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x12c24c900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x127f315a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ba7a240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x6baa80de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x6baa82240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x6baa827d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c24cd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x6c0789340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x6baa83010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x6baa83880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127f31fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x6baa840c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c24d5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ba7aa60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x6baa84930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c24de10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x6baa85250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c24e6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c24ef70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127f32d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c24f870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x127f33650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c250170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c250a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c251300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x127f33f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x6baa85b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127f34820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x6baa86370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ba7aeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ba7b740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ba7c060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x6baa86c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ba7c950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c251c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ba7d1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127f35120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c252350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x6c078c160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x6baa87570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x6baa87e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x6baa88750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c252b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c253420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x6baa89010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x6baa898d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ba7dab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ba7e3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c253d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c254630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c254f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127f328b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127f35930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x6baa8a190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x6baa8a9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x6baa8b220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x6c078bcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x6c078d280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c23a730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x6baa8ba70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127f36200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x6c078da90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x6c078e380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127f374f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c23af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c23b890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c23c190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c23c930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ba7ecd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ba7f530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x6baa8c380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c23d020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c23d940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c23e240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x6baa8cce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x6baa8d5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ba7fe40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127f38150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x6baa8dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ba80760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x6baa8e720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127f38a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ba81030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x6baa8f0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x6baa8f8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ba81880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ba821c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x127f392f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x6c078c940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c23eaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ba82a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x6c078f5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c23f320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c23fb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ba83270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ba83a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c240390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x6baa90200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c240bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127f37b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c241410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c241c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x6baa90a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x6baa91360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x6baa91c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x6baa924a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ba842f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c242540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x6baa92db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x6baa93660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c242e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127f39fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127f3a320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x6baa93f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x6c078fdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ba84dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ba85720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ba85ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ba868c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x6baa94860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ba87190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x6c07906f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ba87a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x6c078eba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ba881a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x6baa95120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c243760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c244050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127f3b440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c244940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c245210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x6baa959d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x6baa96270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x6baa966a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x6baa96b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x6baa97370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x6baa97b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x6baa98a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c245980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x6c0791930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x6c0792480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x12c245c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x127f3bd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c246170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ba6e570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x12ba76060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ba6f660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ba70400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x6c0790f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x6c0793240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x6c0793ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ba88460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127f3c130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127f3cee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127f3d7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c228c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ba88f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c229050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c229500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c229890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c22aca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ba74480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127f3e110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ba89860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x6baa99310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c22b100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c22a1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c22b940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127f3ea40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x6baa9ba70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x6baa9b120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c22c7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ba89c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c22d660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x6c0794930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x6baa9c320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x6baa9c850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x6baa9de30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x6c07951f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x6c0794cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x6c0795f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x6c0796840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c22cb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c22d060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c22e820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127f3ee50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127f7b060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127f7b470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x6c0797120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x6c0795b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x6c07979f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x6c0798be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x6c0799980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127f7c270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127f7c680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ba8b1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x6c07993c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x12c22f5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x6c0799d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c22ebb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c22df70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ba8bcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c230a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ba8c100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x6baa9bf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x6baa9e5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x6baa9ea90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x6c079ab70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127f7ca80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x6c079b3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x6baa9f670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x12ba8c980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x6c079a700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x6baaa00a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x6c079b9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x127f7d7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ba8cf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ba8d740 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  3008.00 MiB\n",
      "llama_init_from_model: KV self size  = 3072.00 MiB, K (f16): 1536.00 MiB, V (f16): 1536.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:      Metal compute buffer size =  1352.00 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =  1352.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1686\n",
      "llama_init_from_model: graph splits = 755 (with bs=512), 3 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\", 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151643', 'tokenizer.ggml.bos_token_id': '151646', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.attention.layer_norm_rms_epsilon': '0.000010', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.embedding_length': '5120', 'qwen2.attention.head_count_kv': '8', 'tokenizer.ggml.add_eos_token': 'false', 'qwen2.context_length': '131072', 'general.type': 'model', 'qwen2.attention.head_count': '40', 'qwen2.feed_forward_length': '13824', 'general.basename': 'DeepSeek-R1-Distill-Qwen', 'qwen2.block_count': '48', 'general.architecture': 'qwen2', 'general.file_type': '15', 'general.size_label': '14B', 'general.name': 'DeepSeek R1 Distill Qwen 14B'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\n",
      "Using chat eos_token: <｜end▁of▁sentence｜>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# path = \"/Users/idks/.ollama/models/blobs/sha256-4824460d29f2058aaf6e1118a63a7a197a09bed509f0e7d4e2efb1ee273b447d\" # llama 3.1 70b\n",
    "# path = \"/Users/idks/.ollama/models/blobs/sha256-4cd576d9aa16961244012223abf01445567b061f1814b57dfef699e4cf8df339\" # deepseek 70b\n",
    "# path = \"/Users/idks/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff\" # llama 3.2 3b\n",
    "# path=\"/Users/idks/.ollama/models/blobs/sha256-fd7b6731c33c57f61767612f56517460ec2d1e2e5a3f0163e0eb3d8d8cb5df20\" # phi4\n",
    "path = \"/Users/idks/.ollama/models/blobs/sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e\" # deepseek 14b\n",
    "\n",
    "\n",
    "llm = LlamaCPP(\n",
    "            model_path=path,\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=16384,\n",
    "            context_window=16384,\n",
    "            generate_kwargs={},\n",
    "            model_kwargs={\"n_gpu_layers\": 1},  # Use GPU acceleration if available\n",
    "            verbose=True,\n",
    "            # function_calling_mode=\"schema\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "# from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "# from llama_index.core.schema import Document\n",
    "# from llama_index.core.llms import ChatMessage, MessageRole\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.agent.openai import OpenAIAgent\n",
    "# from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "class CSVAgent:\n",
    "    def __init__(self, csv_path: str = None, csv_data: pd.DataFrame = None):\n",
    "        \"\"\"Initialize with either a path to a CSV file or a pandas DataFrame.\"\"\"\n",
    "        if csv_data is not None:\n",
    "            self.df = csv_data\n",
    "        elif csv_path is not None:\n",
    "            self.df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either csv_path or csv_data must be provided\")\n",
    "        \n",
    "        self.csv_path = csv_path\n",
    "        self.column_info = self._get_column_info()\n",
    "        \n",
    "        self.llm = llm\n",
    "        \n",
    "        # Create tools\n",
    "        self.tools = [\n",
    "            FunctionTool.from_defaults(fn=self.get_dataframe_info),\n",
    "            FunctionTool.from_defaults(fn=self.get_column_data),\n",
    "            FunctionTool.from_defaults(fn=self.query_data),\n",
    "            FunctionTool.from_defaults(fn=self.filter_data),\n",
    "            FunctionTool.from_defaults(fn=self.group_by_data),\n",
    "            FunctionTool.from_defaults(fn=self.sort_data),\n",
    "            FunctionTool.from_defaults(fn=self.get_statistics),\n",
    "            FunctionTool.from_defaults(fn=self.generate_chart)\n",
    "        ]\n",
    "        \n",
    "        # Create agent\n",
    "        self.agent = ReActAgent.from_tools(\n",
    "            self.tools,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            system_prompt=(\n",
    "                \"You are a helpful assistant that analyzes CSV data. \"\n",
    "                \"Use the available tools to explore, analyze, and visualize the data. \"\n",
    "                \"When using tools that return data, summarize the results in a clear and helpful way. \"\n",
    "                \"For numerical data, consider providing statistical insights. \"\n",
    "                \"For categorical data, consider providing distributions or patterns.\"\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _get_column_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about the columns in the DataFrame.\"\"\"\n",
    "        info = {}\n",
    "        for col in self.df.columns:\n",
    "            info[col] = {\n",
    "                \"dtype\": str(self.df[col].dtype),\n",
    "                \"sample_values\": self.df[col].head(3).tolist(),\n",
    "                \"unique_count\": self.df[col].nunique(),\n",
    "                \"null_count\": self.df[col].isna().sum()\n",
    "            }\n",
    "        return info\n",
    "    \n",
    "    def get_dataframe_info(self) -> str:\n",
    "        \"\"\"Get basic information about the dataframe.\"\"\"\n",
    "        shape = self.df.shape\n",
    "        columns = self.df.columns.tolist()\n",
    "        dtypes = self.df.dtypes.to_dict()\n",
    "        dtypes = {k: str(v) for k, v in dtypes.items()}\n",
    "        \n",
    "        missing_data = self.df.isna().sum().to_dict()\n",
    "        \n",
    "        info = {\n",
    "            \"rows\": shape[0],\n",
    "            \"columns\": shape[1],\n",
    "            \"column_names\": columns,\n",
    "            \"dtypes\": dtypes,\n",
    "            \"missing_values\": missing_data,\n",
    "            \"sample_data\": self.df.head(5).to_dict(orient=\"records\")\n",
    "        }\n",
    "        \n",
    "        return str(info)\n",
    "    \n",
    "    def get_column_data(self, column_name: str) -> str:\n",
    "        \"\"\"Get data from a specific column.\"\"\"\n",
    "        if column_name not in self.df.columns:\n",
    "            return f\"Column '{column_name}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "        \n",
    "        values = self.df[column_name].tolist()\n",
    "        return str(values)\n",
    "    \n",
    "    def query_data(self, query: str) -> str:\n",
    "        \"\"\"Run a pandas query on the dataframe.\"\"\"\n",
    "        try:\n",
    "            result = self.df.query(query)\n",
    "            if len(result) > 10:\n",
    "                return f\"Query returned {len(result)} rows. First 10 rows:\\n{result.head(10).to_string()}\"\n",
    "            return result.to_string()\n",
    "        except Exception as e:\n",
    "            return f\"Error executing query: {str(e)}\"\n",
    "    \n",
    "    def filter_data(self, column: str, value: str, operator: str = \"==\") -> str:\n",
    "        \"\"\"Filter dataframe based on column value.\"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return f\"Column '{column}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "        \n",
    "        try:\n",
    "            # Convert value to appropriate type if numeric\n",
    "            if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            # Handle different operators\n",
    "            if operator == \"==\":\n",
    "                filtered_df = self.df[self.df[column] == value]\n",
    "            elif operator == \"!=\":\n",
    "                filtered_df = self.df[self.df[column] != value]\n",
    "            elif operator == \">\":\n",
    "                filtered_df = self.df[self.df[column] > value]\n",
    "            elif operator == \">=\":\n",
    "                filtered_df = self.df[self.df[column] >= value]\n",
    "            elif operator == \"<\":\n",
    "                filtered_df = self.df[self.df[column] < value]\n",
    "            elif operator == \"<=\":\n",
    "                filtered_df = self.df[self.df[column] <= value]\n",
    "            elif operator == \"contains\":\n",
    "                filtered_df = self.df[self.df[column].astype(str).str.contains(str(value))]\n",
    "            else:\n",
    "                return f\"Unsupported operator: {operator}\"\n",
    "            \n",
    "            if len(filtered_df) > 10:\n",
    "                return f\"Filter returned {len(filtered_df)} rows. First 10 rows:\\n{filtered_df.head(10).to_string()}\"\n",
    "            return filtered_df.to_string()\n",
    "        except Exception as e:\n",
    "            return f\"Error filtering data: {str(e)}\"\n",
    "    \n",
    "    def group_by_data(self, group_cols: str, agg_dict: str) -> str:\n",
    "        \"\"\"Group data by columns and aggregate.\"\"\"\n",
    "        try:\n",
    "            group_cols = [col.strip() for col in group_cols.split(\",\")]\n",
    "            for col in group_cols:\n",
    "                if col not in self.df.columns:\n",
    "                    return f\"Column '{col}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "            \n",
    "            # Parse the aggregation dictionary\n",
    "            import ast\n",
    "            agg_dict = ast.literal_eval(agg_dict)\n",
    "            \n",
    "            result = self.df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "            if len(result) > 10:\n",
    "                return f\"Groupby returned {len(result)} rows. First 10 rows:\\n{result.head(10).to_string()}\"\n",
    "            return result.to_string()\n",
    "        except Exception as e:\n",
    "            return f\"Error in group by operation: {str(e)}\"\n",
    "    \n",
    "    def sort_data(self, columns: str, ascending: bool = True) -> str:\n",
    "        \"\"\"Sort dataframe by specified columns.\"\"\"\n",
    "        try:\n",
    "            sort_cols = [col.strip() for col in columns.split(\",\")]\n",
    "            for col in sort_cols:\n",
    "                if col not in self.df.columns:\n",
    "                    return f\"Column '{col}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "            \n",
    "            result = self.df.sort_values(by=sort_cols, ascending=ascending)\n",
    "            if len(result) > 10:\n",
    "                return f\"Sorted data ({len(result)} rows). First 10 rows:\\n{result.head(10).to_string()}\"\n",
    "            return result.to_string()\n",
    "        except Exception as e:\n",
    "            return f\"Error sorting data: {str(e)}\"\n",
    "    \n",
    "    def get_statistics(self, columns: str = \"all\") -> str:\n",
    "        \"\"\"Get descriptive statistics for numeric columns.\"\"\"\n",
    "        try:\n",
    "            if columns.lower() == \"all\":\n",
    "                numeric_df = self.df.select_dtypes(include=np.number)\n",
    "                if numeric_df.empty:\n",
    "                    return \"No numeric columns found in the dataset.\"\n",
    "                return numeric_df.describe().to_string()\n",
    "            \n",
    "            columns = [col.strip() for col in columns.split(\",\")]\n",
    "            stats_df = pd.DataFrame()\n",
    "            \n",
    "            for col in columns:\n",
    "                if col not in self.df.columns:\n",
    "                    return f\"Column '{col}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "                \n",
    "                if not pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                    return f\"Column '{col}' is not numeric. Can only get statistics for numeric columns.\"\n",
    "                \n",
    "                stats_df[col] = self.df[col]\n",
    "            \n",
    "            return stats_df.describe().to_string()\n",
    "        except Exception as e:\n",
    "            return f\"Error getting statistics: {str(e)}\"\n",
    "    \n",
    "    def generate_chart(self, chart_type: str, x_col: str, y_col: str = None, title: str = \"Chart\") -> str:\n",
    "        \"\"\"Generate a chart based on the data.\"\"\"\n",
    "        try:\n",
    "            # Validate columns\n",
    "            if x_col not in self.df.columns:\n",
    "                return f\"Column '{x_col}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "            \n",
    "            if y_col and y_col not in self.df.columns:\n",
    "                return f\"Column '{y_col}' not found. Available columns: {', '.join(self.df.columns)}\"\n",
    "            \n",
    "            # Create different chart types\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            if chart_type.lower() == \"bar\":\n",
    "                if y_col:\n",
    "                    sns.barplot(x=self.df[x_col], y=self.df[y_col])\n",
    "                else:\n",
    "                    self.df[x_col].value_counts().plot(kind='bar')\n",
    "                \n",
    "            elif chart_type.lower() == \"histogram\":\n",
    "                sns.histplot(self.df[x_col])\n",
    "                \n",
    "            elif chart_type.lower() == \"scatter\":\n",
    "                if not y_col:\n",
    "                    return \"Scatter plot requires both x and y columns.\"\n",
    "                sns.scatterplot(x=self.df[x_col], y=self.df[y_col])\n",
    "                \n",
    "            elif chart_type.lower() == \"line\":\n",
    "                if y_col:\n",
    "                    sns.lineplot(x=self.df[x_col], y=self.df[y_col])\n",
    "                else:\n",
    "                    self.df[x_col].plot(kind='line')\n",
    "                \n",
    "            elif chart_type.lower() == \"box\":\n",
    "                sns.boxplot(x=self.df[x_col])\n",
    "                \n",
    "            elif chart_type.lower() == \"violin\":\n",
    "                if y_col:\n",
    "                    sns.violinplot(x=self.df[x_col], y=self.df[y_col])\n",
    "                else:\n",
    "                    sns.violinplot(y=self.df[x_col])\n",
    "                \n",
    "            else:\n",
    "                return f\"Chart type '{chart_type}' not supported. Supported types: bar, histogram, scatter, line, box, violin\"\n",
    "            \n",
    "            plt.title(title)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the chart temporarily\n",
    "            temp_file = \"temp_chart.png\"\n",
    "            plt.savefig(temp_file)\n",
    "            plt.close()\n",
    "            \n",
    "            return f\"Chart generated and saved as {temp_file}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error generating chart: {str(e)}\"\n",
    "    \n",
    "    def chat(self, message: str) -> str:\n",
    "        \"\"\"Chat with the agent about the CSV data.\"\"\"\n",
    "        response = self.agent.chat(message)\n",
    "        return response.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CSVAgent(csv_path='./csv/reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which day of the month had the highest unique viewership?\n",
      "> Running step c46f1b69-721d-43a1-9df6-5a094355ad27. Step input: Which day of the month had the highest unique viewership?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1095 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   20079.81 ms\n",
      "llama_perf_context_print: prompt eval time =     543.76 ms /    14 tokens (   38.84 ms per token,    25.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55457.79 ms /   814 runs   (   68.13 ms per token,    14.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   56414.02 ms /   828 tokens\n",
      "Llama.generate: 1108 prefix-match hit, remaining 832 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I need to find the day with the highest unique viewership. I'll start by getting some basic information about the dataframe to understand the available data.\n",
      "Action: get_dataframe_info\n",
      "Action Input: {}\n",
      "\u001b[0m\u001b[1;3;34mObservation: {'rows': 2939, 'columns': 8, 'column_names': ['Date', 'Channel Name', 'Session Count', 'Total Viewership Minutes', 'Unique Viewers', 'Avg Session Count', 'Avg Session Duration Per Session', 'Avg Session Duration Per Viewer'], 'dtypes': {'Date': 'object', 'Channel Name': 'object', 'Session Count': 'int64', 'Total Viewership Minutes': 'float64', 'Unique Viewers': 'int64', 'Avg Session Count': 'float64', 'Avg Session Duration Per Session': 'float64', 'Avg Session Duration Per Viewer': 'float64'}, 'missing_values': {'Date': 0, 'Channel Name': 0, 'Session Count': 0, 'Total Viewership Minutes': 0, 'Unique Viewers': 0, 'Avg Session Count': 0, 'Avg Session Duration Per Session': 0, 'Avg Session Duration Per Viewer': 0}, 'sample_data': [{'Date': '01-31-2025', 'Channel Name': 'Channel: 123GO-English  Platform: Anoki  Delivery Region: US', 'Session Count': 132, 'Total Viewership Minutes': 2834.45, 'Unique Viewers': 18, 'Avg Session Count': 7.33, 'Avg Session Duration Per Session': 21.47, 'Avg Session Duration Per Viewer': 157.47}, {'Date': '01-31-2025', 'Channel Name': 'Channel: 123GO-English  Platform: Free TV  Delivery Region: US', 'Session Count': 33, 'Total Viewership Minutes': 126.63, 'Unique Viewers': 29, 'Avg Session Count': 1.14, 'Avg Session Duration Per Session': 3.84, 'Avg Session Duration Per Viewer': 4.37}, {'Date': '01-31-2025', 'Channel Name': 'Channel: 123GO-English  Platform: LG  Delivery Region: AU', 'Session Count': 2826, 'Total Viewership Minutes': 11136.43, 'Unique Viewers': 1866, 'Avg Session Count': 1.51, 'Avg Session Duration Per Session': 3.94, 'Avg Session Duration Per Viewer': 5.97}, {'Date': '01-31-2025', 'Channel Name': 'Channel: 123GO-English  Platform: LG  Delivery Region: IN', 'Session Count': 13837, 'Total Viewership Minutes': 15192.88, 'Unique Viewers': 9036, 'Avg Session Count': 1.53, 'Avg Session Duration Per Session': 1.1, 'Avg Session Duration Per Viewer': 1.68}, {'Date': '01-31-2025', 'Channel Name': 'Channel: 123GO-English  Platform: Netgem TV  Delivery Region: GB', 'Session Count': 223, 'Total Viewership Minutes': 1312.07, 'Unique Viewers': 107, 'Avg Session Count': 2.08, 'Avg Session Duration Per Session': 5.88, 'Avg Session Duration Per Viewer': 12.26}]}\n",
      "\u001b[0m> Running step 790f5c80-4c4d-4263-ad5b-08071fbca59c. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   20079.81 ms\n",
      "llama_perf_context_print: prompt eval time =   14595.47 ms /   832 tokens (   17.54 ms per token,    57.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19187.98 ms /   254 runs   (   75.54 ms per token,    13.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   33878.45 ms /  1086 tokens\n",
      "Llama.generate: 1939 prefix-match hit, remaining 118 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: Now that I have the dataframe info, I see that there's a 'Unique Viewers' column which is exactly what I need. Next, I should filter the data to find the maximum unique viewers for each date.\n",
      "Action: filter_data\n",
      "Action Input: {'column': 'Unique Viewers', 'value': 'max'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Empty DataFrame\n",
      "Columns: [Date, Channel Name, Session Count, Total Viewership Minutes, Unique Viewers, Avg Session Count, Avg Session Duration Per Session, Avg Session Duration Per Viewer]\n",
      "Index: []\n",
      "\u001b[0m> Running step b550c79e-21a2-40f2-9098-7c7525c9681a. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   20079.81 ms\n",
      "llama_perf_context_print: prompt eval time =    3284.67 ms /   118 tokens (   27.84 ms per token,    35.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25957.96 ms /   338 runs   (   76.80 ms per token,    13.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   29373.07 ms /   456 tokens\n",
      "Llama.generate: 2056 prefix-match hit, remaining 115 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: It seems that using filter_data with 'Unique Viewers' and 'max' didn't return any rows. Maybe I should try a different approach. Perhaps I should group the data by date and then find the maximum unique viewers for each date.\n",
      "Action: group_by_data\n",
      "Action Input: {'group_cols': 'Date', 'agg_dict': AttributedDict([('Unique Viewers', 'max')])}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error in group by operation: malformed node or string: AttributedDict([('Unique Viewers', 'max')])\n",
      "\u001b[0m> Running step f529bd63-5ce9-4ded-97ed-3a224da1e8bc. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   20079.81 ms\n",
      "llama_perf_context_print: prompt eval time =    3302.84 ms /   115 tokens (   28.72 ms per token,    34.82 tokens per second)\n",
      "llama_perf_context_print:        eval time = 1407407.71 ms / 14212 runs   (   99.03 ms per token,    10.10 tokens per second)\n",
      "llama_perf_context_print:       total time = 1470047.07 ms / 14327 tokens\n",
      "Llama.generate: 2170 prefix-match hit, remaining 300 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: There was an error in the group_by_data tool call. It seems the agg_dict wasn't properly formatted. I need to provide the aggregation dictionary correctly.\n",
      "Action: group_by_data\n",
      "Action Input: {'group_cols': 'Date', 'agg_dict': '{\"Unique Viewers\": \"max\"}'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Groupby returned 30 rows. First 10 rows:\n",
      "         Date  Unique Viewers\n",
      "0  01-31-2025           18618\n",
      "1  02-01-2025           23062\n",
      "2  02-02-2025           22805\n",
      "3  02-03-2025           17549\n",
      "4  02-04-2025           16881\n",
      "5  02-05-2025           18179\n",
      "6  02-06-2025           17252\n",
      "7  02-07-2025           25083\n",
      "8  02-08-2025           24766\n",
      "9  02-09-2025           22453\n",
      "\u001b[0m> Running step 7bebad26-9750-4a0e-86b5-ba23ac632e43. Step input: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   20079.81 ms\n",
      "llama_perf_context_print: prompt eval time =    6062.17 ms /   300 tokens (   20.21 ms per token,    49.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   60498.41 ms /   823 runs   (   73.51 ms per token,    13.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   66948.81 ms /  1123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: I have the grouped data by date with the maximum unique viewers. Now, I need to find which date has the highest value in the 'Unique Viewers' column.\n",
      "Action: get_statistics\n",
      "Action Input: {'columns': 'Unique Viewers'}\n",
      "\u001b[0m\u001b[1;3;34mObservation:        Unique Viewers\n",
      "count     2939.000000\n",
      "mean      1792.731541\n",
      "std       3563.543913\n",
      "min          1.000000\n",
      "25%         19.000000\n",
      "50%        141.000000\n",
      "75%       1690.500000\n",
      "max      25083.000000\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Reached max iterations.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 262\u001b[39m, in \u001b[36mCSVAgent.chat\u001b[39m\u001b[34m(self, message)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    261\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chat with the agent about the CSV data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/callbacks/utils.py:41\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:692\u001b[39m, in \u001b[36mAgentRunner.chat\u001b[39m\u001b[34m(self, message, chat_history, tool_choice)\u001b[39m\n\u001b[32m    687\u001b[39m     tool_choice = \u001b[38;5;28mself\u001b[39m.default_tool_choice\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    689\u001b[39m     CBEventType.AGENT_STEP,\n\u001b[32m    690\u001b[39m     payload={EventPayload.MESSAGES: [message]},\n\u001b[32m    691\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatResponseMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[32m    699\u001b[39m     e.on_end(payload={EventPayload.RESPONSE: chat_response})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:624\u001b[39m, in \u001b[36mAgentRunner._chat\u001b[39m\u001b[34m(self, message, chat_history, tool_choice, mode)\u001b[39m\n\u001b[32m    621\u001b[39m dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     cur_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output.is_last:\n\u001b[32m    629\u001b[39m         result_output = cur_step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/agent/runner/base.py:420\u001b[39m, in \u001b[36mAgentRunner._run_step\u001b[39m\u001b[34m(self, task_id, step, input, mode, **kwargs)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == ChatResponseMode.WAIT:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     cur_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == ChatResponseMode.STREAM:\n\u001b[32m    422\u001b[39m     cur_step_output = \u001b[38;5;28mself\u001b[39m.agent_worker.stream_step(step, task, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/callbacks/utils.py:41\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/agent/react/step.py:820\u001b[39m, in \u001b[36mReActAgentWorker.run_step\u001b[39m\u001b[34m(self, step, task, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mrun_step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n\u001b[32m    819\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/agent/react/step.py:578\u001b[39m, in \u001b[36mReActAgentWorker._run_step\u001b[39m\u001b[34m(self, step, task)\u001b[39m\n\u001b[32m    574\u001b[39m reasoning_steps, is_done = \u001b[38;5;28mself\u001b[39m._process_actions(\n\u001b[32m    575\u001b[39m     task, tools, output=chat_response\n\u001b[32m    576\u001b[39m )\n\u001b[32m    577\u001b[39m task.extra_state[\u001b[33m\"\u001b[39m\u001b[33mcurrent_reasoning\u001b[39m\u001b[33m\"\u001b[39m].extend(reasoning_steps)\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m agent_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcurrent_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msources\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_done:\n\u001b[32m    582\u001b[39m     task.extra_state[\u001b[33m\"\u001b[39m\u001b[33mnew_memory\u001b[39m\u001b[33m\"\u001b[39m].put(\n\u001b[32m    583\u001b[39m         ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/ai/agentic-framework/venv/lib/python3.11/site-packages/llama_index/core/agent/react/step.py:437\u001b[39m, in \u001b[36mReActAgentWorker._get_response\u001b[39m\u001b[34m(self, current_reasoning, sources)\u001b[39m\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo reasoning steps were taken.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current_reasoning) == \u001b[38;5;28mself\u001b[39m._max_iterations:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mReached max iterations.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_reasoning[-\u001b[32m1\u001b[39m], ResponseReasoningStep):\n\u001b[32m    440\u001b[39m     response_step = cast(ResponseReasoningStep, current_reasoning[-\u001b[32m1\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: Reached max iterations."
     ]
    }
   ],
   "source": [
    "def main():    \n",
    "    # Example queries\n",
    "    queries = [\n",
    "        # \"Which Channel Name has the most Total Viewership Minutes\",\n",
    "        \"Which day of the month had the highest unique viewership?\",\n",
    "        \"Which week of the month saw the most significant increase in viewership?\"\n",
    "        # \"What does this dataset contain?\",\n",
    "        # \"What's the average salary by department?\",\n",
    "        # \"Who has the highest salary?\",\n",
    "        # \"Show me a bar chart of salaries by department\",\n",
    "        # \"Is there a correlation between age and salary?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuestion: {query}\")\n",
    "        response = agent.chat(query)\n",
    "        print(f\"Answer: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
